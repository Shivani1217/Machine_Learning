{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KNN & PCA | Assignment\n"
      ],
      "metadata": {
        "id": "nBrox8hkv7j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "- K-Nearest Neighbors (KNN) is a **supervised machine learning algorithm** that makes predictions by looking at the **“K closest” data points** (neighbors) to a new input.\n",
        "\n",
        "It is called **“lazy learning”** because it **does not build a model** during training. It simply **stores the training data** and uses it during prediction time.\n",
        "\n",
        "-  How KNN Works (Step-by-Step)\n",
        "\n",
        "For a new data point (test point), KNN does this:\n",
        "\n",
        "1. **Choose a value of K** (example: K = 3, 5, 7)\n",
        "2. **Calculate distance** between the new point and all training points\n",
        "   (common distance: **Euclidean distance**)\n",
        "3. **Pick the K nearest points**\n",
        "4. Use those neighbors to make the prediction:\n",
        "\n",
        "   * **Majority vote → Classification**\n",
        "   * **Average value → Regression**\n",
        "- Distance Measures Used in KNN\n",
        "\n",
        "Most common:\n",
        " 1) Euclidean Distance\n",
        "\n",
        "[\n",
        "d = \\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}\n",
        "]\n",
        "\n",
        "Other distances:\n",
        "\n",
        "* **Manhattan distance**\n",
        "* **Minkowski distance**\n",
        "* **Cosine similarity** (often used in text problems)\n",
        "\n",
        " KNN for Classification (Example)\n",
        "\n",
        "### Goal: Predict a class (like Yes/No, Spam/Not Spam)\n",
        "\n",
        "### How prediction is made:\n",
        "\n",
        "* Take **K nearest neighbors**\n",
        "* Check their labels\n",
        "* The **most common class wins** (majority voting)\n",
        "\n",
        "### Example:\n",
        "\n",
        "If K = 5 neighbors have labels:\n",
        "✅ {A, A, B, A, B}\n",
        "\n",
        "Then prediction = **A** (because A appears 3 times)\n",
        "\n",
        "\n",
        "\n",
        "#  KNN for Regression (Example)\n",
        "\n",
        "### Goal: Predict a number (like price, salary, marks)\n",
        "\n",
        "### How prediction is made:\n",
        "\n",
        "* Take **K nearest neighbors**\n",
        "* Take the **average of their values**\n",
        "\n",
        "### Example:\n",
        "\n",
        "If K = 3 neighbors have values:\n",
        "{200, 220, 210}\n",
        "\n",
        "Prediction:\n",
        "[\n",
        "\\frac{200+220+210}{3} = 210\n",
        "]\n",
        "\n",
        "-Choosing the Right K Value\n",
        "\n",
        "* **Small K (like 1 or 3):**\n",
        "\n",
        "  * Very sensitive to noise\n",
        "  * Can overfit\n",
        "\n",
        "* **Large K (like 15 or 25):**\n",
        "\n",
        "  * More stable\n",
        "  * Can underfit\n",
        "\n",
        "Usually, we test multiple K values using **cross-validation**.\n",
        " - Advantages of KNN\n",
        "\n",
        "✔ Simple and easy to understand\n",
        "✔ Works well for small datasets\n",
        "✔ No training time (fast training)\n",
        "\n",
        "-  Disadvantages of KNN\n",
        "\n",
        " Slow prediction for large datasets (because it checks all points). Sensitive to irrelevant features . Needs **feature scaling** (important!)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WHkypt8dv_1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "- The **Curse of Dimensionality** means:\n",
        " When the **number of features (dimensions)** becomes very large, many machine learning algorithms (especially distance-based ones like **KNN**) start performing **poorly**.\n",
        "\n",
        "\n",
        "\n",
        "## Why is it called a “curse”?\n",
        "\n",
        "Because in high dimensions:\n",
        "\n",
        "### 1) **Data becomes sparse**\n",
        "\n",
        "Even if you have thousands of rows, in a high-dimensional space the points are **far apart** and the space is mostly empty.\n",
        "\n",
        "So KNN struggles to find “truly close” neighbors.\n",
        "\n",
        "\n",
        "\n",
        "## How it affects KNN performance\n",
        "\n",
        "KNN depends completely on **distance** (Euclidean/Manhattan etc.).\n",
        "In high dimensions:\n",
        "\n",
        "### 1) **Distances become less meaningful**\n",
        "\n",
        "The distance between the nearest and farthest neighbors becomes almost the same.\n",
        "\n",
        "So KNN can’t clearly decide which points are “nearest”.\n",
        "\n",
        " Example idea:\n",
        "\n",
        "* In 2D, nearest points are clearly close.\n",
        "* In 100D, almost all points look similarly far.\n",
        "\n",
        "\n",
        "\n",
        "### 2) **More noise features reduce accuracy**\n",
        "\n",
        "If many features are irrelevant, KNN includes them in distance calculation, making wrong neighbors appear “close”.\n",
        "\n",
        "This reduces classification/regression quality.\n",
        "\n",
        "\n",
        "### 3) **Prediction becomes slower**\n",
        "\n",
        "KNN must compute distance from the test point to **all training points**.\n",
        "More features = more calculations = slower prediction.\n",
        "\n",
        "\n",
        "\n",
        "### 4) **Needs much more data**\n",
        "\n",
        "To cover the space properly, high dimensions need **huge data**.\n",
        "Otherwise KNN overfits or becomes unstable.\n",
        "\n",
        "\n",
        "## Result on KNN\n",
        "\n",
        "**Accuracy decreases**\n",
        " **Prediction time increases**\n",
        " **Neighbors become unreliable**\n",
        "\n",
        "\n",
        "##  How to fix / reduce the problem\n",
        "\n",
        "✔ **Feature scaling** (must for KNN)\n",
        "✔ **Feature selection** (remove useless features)\n",
        "✔ **Dimensionality reduction** (PCA, t-SNE for visualization)\n",
        "✔ **Use smaller number of important features**\n",
        "✔ Try other models (Decision Trees, Random Forest, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "MTbgLvOQxDUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "- ## What is PCA (Principal Component Analysis)?\n",
        "\n",
        "**PCA** is a **dimensionality reduction technique** that converts your original features into a **new set of features** called **principal components**.\n",
        "\n",
        "These principal components:\n",
        "\n",
        "* are **combinations of original features**\n",
        "* are **uncorrelated (independent)**\n",
        "* capture the **maximum variance (information)** in the data\n",
        "\n",
        " Goal: **Reduce features but keep most of the important information.**\n",
        "\n",
        "\n",
        "## How PCA works\n",
        "\n",
        "PCA finds new directions (axes) in the data such that:\n",
        "\n",
        "1. **PC1 (1st Principal Component)** captures the **most variance**\n",
        "2. **PC2** captures the **2nd most variance** (and is perpendicular to PC1)\n",
        "3. And so on…\n",
        "\n",
        "Then we keep only the **top components** (like 2 or 3 instead of 20 features).\n",
        "\n",
        "  Example\n",
        "\n",
        "Suppose you have **10 features**.\n",
        "\n",
        "After PCA, you may reduce them to **3 principal components** while still keeping **95% of the data information**.\n",
        "\n",
        "So instead of:\n",
        " (X1, X2, X3, … X10)\n",
        "\n",
        "You use:\n",
        " (PC1, PC2, PC3)\n",
        "\n",
        "\n",
        "\n",
        "#  PCA vs Feature Selection (Main Difference)\n",
        "\n",
        "| Feature Selection                            | PCA                                             |\n",
        "| -------------------------------------------- | ----------------------------------------------- |\n",
        "| Selects the **best original features**       | Creates **new features (principal components)** |\n",
        "| Keeps features like X1, X4, X7               | Makes new features like PC1 = 0.5X1 + 0.3X2 + … |\n",
        "| More **interpretable**                       | Less interpretable (hard to explain)            |\n",
        "| Can improve model performance                | Reduces noise + helps with multicollinearity    |\n",
        "| Works well when some features are irrelevant | Works well when features are correlated         |\n",
        "\n",
        "\n",
        "\n",
        "##  Key Point\n",
        "\n",
        "###  Feature Selection:\n",
        "\n",
        "✔ Removes unnecessary features\n",
        "✔ Keeps real features\n",
        "✔ Easy to explain\n",
        "\n",
        "###  PCA:\n",
        "\n",
        "✔ Compresses features into fewer components\n",
        "✔ Useful when features are correlated\n",
        "✔ Helps reduce curse of dimensionality\n",
        "\n",
        "\n",
        "\n",
        "## When to use PCA?\n",
        "\n",
        "Use PCA when:\n",
        "\n",
        "* You have **many features**\n",
        "* Features are **highly correlated**\n",
        "* You want faster models and less overfitting\n",
        "* You want to reduce dimensionality for algorithms like **KNN, SVM**\n",
        "\n"
      ],
      "metadata": {
        "id": "Dq09sP23xj6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "- In PCA, **eigenvectors and eigenvalues** come from the **covariance matrix** (or correlation matrix) of the dataset, and they tell PCA:\n",
        "\n",
        " **Which directions to project the data on**\n",
        " **How much information (variance) each direction contains**\n",
        "\n",
        "\n",
        "\n",
        "##  Eigenvectors in PCA (What they mean)\n",
        "\n",
        "**Eigenvectors** represent the **principal components (new axes/directions)**.\n",
        "\n",
        " Think of an eigenvector as a direction in the feature space where the data varies the most.\n",
        "\n",
        "So in PCA:\n",
        "\n",
        "* **Eigenvector 1 → PC1 direction**\n",
        "* **Eigenvector 2 → PC2 direction**\n",
        "* etc.\n",
        "\n",
        " PCA projects the data onto these eigenvectors.\n",
        "\n",
        "\n",
        "##  Eigenvalues in PCA (What they mean)\n",
        "\n",
        "**Eigenvalues** tell **how much variance (information)** is captured along the corresponding eigenvector.\n",
        "\n",
        " Bigger eigenvalue = more important principal component.\n",
        "\n",
        "Example:\n",
        "\n",
        "* Eigenvalue for PC1 = 5.2 (high variance)\n",
        "* Eigenvalue for PC2 = 1.1\n",
        "* Eigenvalue for PC3 = 0.2 (very low variance)\n",
        "\n",
        "So PC1 is the most useful.\n",
        "\n",
        "\n",
        "##  Why are eigenvalues & eigenvectors important in PCA?\n",
        "\n",
        "### 1) They decide the **new feature directions**\n",
        "\n",
        "Eigenvectors define the **principal components** (PCs).\n",
        "\n",
        "### 2) They decide which components to keep\n",
        "\n",
        "Eigenvalues help you choose how many PCs to keep.\n",
        "\n",
        "Common method:\n",
        " Keep the components with **highest eigenvalues**\n",
        "\n",
        "\n",
        "##  Explained Variance Ratio\n",
        "\n",
        "PCA uses eigenvalues to calculate:\n",
        "\n",
        "[\n",
        "\\text{Explained Variance Ratio} = \\frac{\\lambda_i}{\\sum \\lambda}\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* ( \\lambda_i ) = eigenvalue of component i\n",
        "\n",
        "This tells what % of information each PC contains.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0C8MzxZ8yOIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "- KNN and PCA work really well together because **PCA makes the data easier for KNN to handle**.\n",
        "\n",
        "###  Why they complement each other\n",
        "\n",
        "KNN is a **distance-based algorithm**, so its performance depends heavily on:\n",
        "\n",
        "* meaningful distances\n",
        "* fewer noisy/irrelevant features\n",
        "* faster distance calculations\n",
        "\n",
        "PCA helps with exactly these points.\n",
        "\n",
        "\n",
        "##  How PCA helps KNN in one pipeline\n",
        "\n",
        "### 1) Reduces the Curse of Dimensionality\n",
        "\n",
        "In high dimensions, distances become less reliable, so KNN struggles.\n",
        "\n",
        " PCA reduces dimensions → distances become more meaningful → KNN becomes more accurate.\n",
        "\n",
        "\n",
        "### 2) Removes noise and redundant features\n",
        "\n",
        "If features are highly correlated or contain noise, KNN can pick wrong neighbors.\n",
        "\n",
        " PCA combines correlated features into fewer strong components → cleaner input for KNN.\n",
        "\n",
        "\n",
        "\n",
        "### 3) Makes KNN faster\n",
        "\n",
        "KNN calculates distance from each test point to all training points.\n",
        "\n",
        " Fewer dimensions = fewer computations = faster prediction.\n",
        "\n",
        "\n",
        "### 4) Can reduce overfitting\n",
        "\n",
        "Too many features can make KNN sensitive to small variations.\n",
        "\n",
        "PCA keeps only important variance → smoother decision boundaries.\n",
        "\n",
        "\n",
        "##  Typical PCA + KNN Pipeline\n",
        "\n",
        "**Correct order:**\n",
        "\n",
        "1. **Scale the data** (important!)\n",
        "2. Apply **PCA**\n",
        "3. Apply **KNN**\n",
        "\n",
        "Example pipeline:\n",
        "`StandardScaler → PCA → KNN`\n",
        "\n"
      ],
      "metadata": {
        "id": "O5Q6z6TuyvqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases."
      ],
      "metadata": {
        "id": "y6x14mIWzPSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSt_WRzbv3VC",
        "outputId": "08c36279-94fc-47e1-979d-dd53b71c779b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT scaling: 0.8055555555555556\n",
            "Accuracy WITH scaling   : 0.9722222222222222\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 1) KNN WITHOUT feature scaling\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scale.fit(X_train, y_train)\n",
        "acc_no_scale = knn_no_scale.score(X_test, y_test)\n",
        "\n",
        "# 2) KNN WITH feature scaling (StandardScaler)\n",
        "knn_scaled = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "knn_scaled.fit(X_train, y_train)\n",
        "acc_scaled = knn_scaled.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy WITHOUT scaling:\", acc_no_scale)\n",
        "print(\"Accuracy WITH scaling   :\", acc_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. : Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component."
      ],
      "metadata": {
        "id": "waB6aa4lzoeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Scale features before PCA\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "evr = pca.explained_variance_ratio_\n",
        "\n",
        "print(\"Explained Variance Ratio of each Principal Component:\\n\")\n",
        "for i, ratio in enumerate(evr, start=1):\n",
        "    print(f\"PC{i}: {ratio:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfTox6Ulzrx7",
        "outputId": "c5e093df-2b7f-4ff2-f8ea-b3ebd6df654d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "\n",
            "PC1: 0.361988\n",
            "PC2: 0.192075\n",
            "PC3: 0.111236\n",
            "PC4: 0.070690\n",
            "PC5: 0.065633\n",
            "PC6: 0.049358\n",
            "PC7: 0.042387\n",
            "PC8: 0.026807\n",
            "PC9: 0.022222\n",
            "PC10: 0.019300\n",
            "PC11: 0.017368\n",
            "PC12: 0.012982\n",
            "PC13: 0.007952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "K9q8MHwCzzbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 1) KNN on ORIGINAL dataset (with scaling)\n",
        "knn_original = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "knn_original.fit(X_train, y_train)\n",
        "acc_original = knn_original.score(X_test, y_test)\n",
        "\n",
        "# 2) KNN on PCA-transformed dataset (Top 2 components)\n",
        "knn_pca2 = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=2)),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "knn_pca2.fit(X_train, y_train)\n",
        "acc_pca2 = knn_pca2.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy on ORIGINAL dataset (scaled):\", acc_original)\n",
        "print(\"Accuracy on PCA dataset (Top 2 PCs)   :\", acc_pca2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWZ5mAeLz1Ko",
        "outputId": "7f55bad0-4ed0-484d-e1ee-20b20d305674"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on ORIGINAL dataset (scaled): 0.9722222222222222\n",
            "Accuracy on PCA dataset (Top 2 PCs)   : 0.9166666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "g4Xu549tz7ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data (KNN is sensitive to feature scales)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Compare different distance metrics\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "print(\"KNN Classification Results (k=5):\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "for metric in metrics:\n",
        "    # Initialize and train the KNN classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Metric: {metric:10} | Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbBx1avu0IEt",
        "outputId": "5da9c16d-5f5d-4f8d-be54-dfcfc1bb146d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classification Results (k=5):\n",
            "-----------------------------------\n",
            "Metric: euclidean  | Accuracy: 0.9444\n",
            "Metric: manhattan  | Accuracy: 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n"
      ],
      "metadata": {
        "id": "Ec0yr7Vo0wTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer -  In high-dimensional biomedical contexts like gene expression analysis, where features (thousands of genes) vastly outnumber samples (patients), a pipeline combining Principal Component Analysis (PCA) and K-Nearest Neighbors (KNN) is a standard robust solution to combat the \"curse of dimensionality\" and prevent overfitting.\n",
        "1. PCA for Dimensionality Reduction\n",
        "PCA transforms the high-dimensional gene space into a set of uncorrelated linear combinations called Principal Components (PCs). In biomedical data, this filters out technical noise and redundant gene correlations, concentrating the most significant biological variations into the first few components.\n",
        "2. Deciding Components to Keep Determining the number of components (\\(k\\)) is critical for balancing information retention against noise reduction: Cumulative Explained Variance: A common threshold is to retain enough components to explain 95% of the total variance.Scree Plot (Elbow Method): Plot eigenvalues in descending order and identify the \"elbow\" point where adding more components yields diminishing returns in variance captured.Kaiser Criterion: Retain only components with an eigenvalue greater than 1.0, as these explain more variance than any single original feature.\n",
        "3. KNN Classification Post-Reduction\n",
        "Once reduced, the data is projectable into a lower-dimensional space (e.g., from 10,000 genes to 50 PCs). KNN then classifies new patients by measuring the Euclidean distance to their \\(K\\) nearest neighbors in this \"cleaner\" PC space. This avoids the distance-metric breakdown that occurs in high-dimensional spaces.\n",
        "4. Evaluation Strategy Cross-Validation: Use k-fold cross-validation (e.g., 5-fold or 10-fold) to ensure the model generalizes across small sample sizes.Clinical Metrics: Beyond accuracy, prioritize Sensitivity (identifying true cancer cases) and Specificity (avoiding false alarms), which are vital in biomedical decisions.Confusion Matrix: Use this to visualize specific misclassifications between cancer subtypes.\n",
        "5. Stakeholder Justification\n",
        "Robustness against Noise: PCA removes \"noisy\" genes that don't vary across patients, ensuring the model focuses only on biologically relevant signals.\n",
        "Prevention of Overfitting: By reducing the feature count, we avoid the model \"memorizing\" specific training samples, making it more reliable for future patients.\n",
        "Computational Efficiency: PCA-KNN runs significantly faster than deep learning or full-feature models, which is crucial for clinical deployment."
      ],
      "metadata": {
        "id": "C7c0D4qB1ID6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load and Scale Data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 2. PCA - Decide components (Explain 95% variance)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(f\"Original features: {X.shape[1]}\")\n",
        "print(f\"Reduced features (PCs): {pca.n_components_}\")\n",
        "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.4f}\")\n",
        "\n",
        "# 3. Train KNN on PCs\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# 4. Evaluate\n",
        "cv_scores = cross_val_score(knn, X_train_pca, y_train, cv=5)\n",
        "test_accuracy = knn.score(X_test_pca, y_test)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Cross-Validation Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUJ-SEgD1t9x",
        "outputId": "f1aa7af5-0d83-406e-8d6b-8072a3a4f3c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original features: 30\n",
            "Reduced features (PCs): 10\n",
            "Total variance explained: 0.9511\n",
            "------------------------------\n",
            "Cross-Validation Mean Accuracy: 0.9538\n",
            "Test Accuracy: 0.9561\n"
          ]
        }
      ]
    }
  ]
}