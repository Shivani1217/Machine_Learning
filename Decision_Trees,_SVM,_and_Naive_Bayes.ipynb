{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "- Information Gain is a metric used in Decision Trees to measure how much uncertainty (entropy) is reduced after splitting a dataset on a particular feature. It helps determine the best feature to split the data at each node of the tree.\n",
        "\n",
        "- Information Gain is calculated as the difference between the entropy of the parent node and the weighted average entropy of the child nodes after the split. The feature with the highest Information Gain is selected for splitting because it results in the purest subsets."
      ],
      "metadata": {
        "id": "nh4AWNpPrc5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the difference between Gini Impurity and Entropy?\n",
        "- Gini Impurity:\n",
        "\n",
        "formula : 1−∑p\n",
        "\n",
        "Computation: fatser\n",
        "\n",
        "usages: deafault in CART\n",
        "\n",
        "Interpretation: Measures probability of misclassification\n",
        "\n",
        "Sensitivity: Less sensitive to changes\n",
        "\n",
        "- Entropy\n",
        "\n",
        "formula : -∑plog2​p\n",
        "\n",
        "Computation: slower\n",
        "\n",
        "usages: used in ID3\n",
        "\n",
        "Interpretation: Measures randomness\n",
        "\n",
        "Sensitivity: More sensitive\n",
        "\n",
        "\n",
        "- Use Case:\n",
        "\n",
        "Gini Impurity is preferred when speed is important.\n",
        "\n",
        "Entropy is preferred when more precise splits are needed."
      ],
      "metadata": {
        "id": "lQQmGEK3sT6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is Pre-Pruning in Decision Trees?\n",
        "- Pre-Pruning is a technique used to stop the growth of a Decision Tree early to prevent overfitting. It restricts tree growth by setting conditions such as maximum depth, minimum samples per node, or minimum impurity decrease.\n",
        "\n",
        "- By limiting complexity during training, pre-pruning improves generalization and reduces computation time."
      ],
      "metadata": {
        "id": "72JvHX7luciO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.ython program to train a Decision Tree Classifier using Gini Impurity\n"
      ],
      "metadata": {
        "id": "grbfonCdusGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree using Gini\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWjOQVkSuvxv",
        "outputId": "89ffa4ca-841a-450c-8257-92fd823c6a3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.01667014 0.         0.90614339 0.07718647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is a Support Vector Machine (SVM)?\n",
        "- A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression. It works by finding an optimal hyperplane that best separates data points of different classes while maximizing the margin between them.\n",
        "\n",
        "- Support vectors are the data points closest to the hyperplane and influence its position."
      ],
      "metadata": {
        "id": "YnSLEFF2u8n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is the Kernel Trick in SVM?\n",
        "- The Kernel Trick is a method used in SVM to transform non-linearly separable data into a higher-dimensional space where it becomes linearly separable. This transformation is done implicitly without computing the coordinates of the data in higher dimensions.\n",
        "\n",
        "Common kernels include:\n",
        "\n",
        "- Linear\n",
        "\n",
        "- Polynomial\n",
        "\n",
        "- Radial Basis Function (RBF)\n",
        "\n",
        "- Sigmoid"
      ],
      "metadata": {
        "id": "U-CZ1r3QvDSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Python program to compare Linear and RBF SVM on Wine dataset"
      ],
      "metadata": {
        "id": "INlf3zh8vQmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear SVM\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "# RBF SVM\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", linear_acc)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqpK7a4evV0z",
        "outputId": "c9c0663a-db7a-4011-c6d1-48a449436b96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0\n",
            "RBF Kernel Accuracy: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "- Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It assumes that all features are conditionally independent given the class label.\n",
        "\n",
        "- It is called “Naïve” because this independence assumption is rarely true in real-world data, yet the algorithm performs well in many practical applications."
      ],
      "metadata": {
        "id": "RITdU5xKvdAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes?\n",
        "- Gaussian NB is a Continous data types and used for Medical .Numerical data.\n",
        "\n",
        "- Multinomial NB is a Count-based data type and used for Text classification.\n",
        "\n",
        "- Bernoulli NB is a\n",
        "Binary data types and used for Spam detection.\n",
        "\n",
        "- Gaussian NB assumes normal distribution\n",
        "\n",
        "- Multinomial NB works with word frequencies\n",
        "\n",
        "- Bernoulli NB works with presence/absence of features"
      ],
      "metadata": {
        "id": "oA_uVsVOvlTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Gaussian Naïve Bayes on Breast Cancer Dataset"
      ],
      "metadata": {
        "id": "FvJTxGfJwR4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2zlVqD0wT53",
        "outputId": "2add56f7-9bf3-47c5-ffc5-aff46fc11e79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}